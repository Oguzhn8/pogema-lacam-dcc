{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a9c7df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def load_and_normalize_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.json_normalize(data)\n",
    "    return df\n",
    "\n",
    "def add_algorithm_prefix(df):\n",
    "    algorithm_name = df['algorithm'].iloc[0]\n",
    "    metric_columns = [col for col in df.columns if col.startswith('metrics.')]\n",
    "    new_column_names = {col: f\"{algorithm_name}_{col.split('.')[-1]}\" for col in metric_columns}\n",
    "    df.rename(columns=new_column_names, inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_combined_df(dataset, path):\n",
    "    json_files = glob.glob(f'{path}/{dataset}/*.json')\n",
    "    dataframes = [load_and_normalize_json(file) for file in json_files]\n",
    "    dataframes = [add_algorithm_prefix(df) for df in dataframes]\n",
    "    combined_df = dataframes[0]\n",
    "    for df in dataframes[1:]:\n",
    "        if dataset == '05-puzzles':\n",
    "            combined_df = pd.merge(combined_df, df, on=['env_grid_search.num_agents', 'env_grid_search.seed', 'env_grid_search.map_name'], suffixes=('', '_dup'))\n",
    "        elif dataset == '03-warehouse':\n",
    "            combined_df = pd.merge(combined_df, df, on=['env_grid_search.num_agents', 'env_grid_search.seed'], suffixes=('', '_dup'))\n",
    "        elif dataset in ['01-random', '02-mazes', '04-movingai']:\n",
    "            combined_df = pd.merge(combined_df, df, on=['env_grid_search.num_agents', 'env_grid_search.map_name'], suffixes=('', '_dup'))\n",
    "        elif dataset in ['06-pathfinding']:\n",
    "            combined_df = pd.merge(combined_df, df, on=['env_grid_search.seed', 'env_grid_search.map_name'], suffixes=('', '_dup'))\n",
    "\n",
    "    # Drop duplicate columns resulting from the merge\n",
    "    combined_df = combined_df.loc[:, ~combined_df.columns.str.endswith('_dup')]\n",
    "    if 'algorithm' in combined_df.columns:\n",
    "        combined_df.drop(columns=['algorithm'], inplace=True)\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def add_congestion(data_dict, algos, combined_df, path_to_maps):\n",
    "    with open(f'{path_to_maps}/maps.yaml', 'r') as f:\n",
    "        maps = yaml.safe_load(f)\n",
    "    traversable_cells = {}\n",
    "    for m in maps:\n",
    "        cells = 0\n",
    "        for i in maps[m]:\n",
    "            if i in ['.', '@', '&', '$', '!']:\n",
    "                cells += 1\n",
    "        traversable_cells[m] = cells\n",
    "    num_agents = combined_df['env_grid_search.num_agents'].max()\n",
    "    filtered_df = combined_df[combined_df['env_grid_search.num_agents'] == combined_df['env_grid_search.num_agents'].max()]\n",
    "    for algo in algos:\n",
    "        density = []\n",
    "        if len(traversable_cells) == 1:\n",
    "            for index, row in filtered_df.iterrows():\n",
    "                density.append((float(num_agents)/cells)/row[f'{algo}_avg_agents_density'])\n",
    "        else:\n",
    "            for index, row in filtered_df.iterrows():\n",
    "                density.append((float(num_agents)/traversable_cells[row['env_grid_search.map_name']])/row[f'{algo}_avg_agents_density'])\n",
    "        data_dict[algo]['Congestion'] = np.array(density).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a25017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0200341285697183\n",
      "1.0492705948333791\n",
      "1.066230461330829\n"
     ]
    }
   ],
   "source": [
    "random_list_2 = [84]\n",
    "random_list_1 = [24,32,40]\n",
    "#random_list_3 = [ 32, 64, 96, 128, 160, 192 ]\n",
    "[232,256 ]\n",
    "random_list_4 = [ 64, 128, 192,256]\n",
    "random_list = random_list_1  \n",
    "raw_Data_path = \"/mnt/c/Users/Oguzhan Karaarslan/Desktop/repos/pogema-lacam-dcc/raw_data_MAPF\"\n",
    "map_type = '01-random'\n",
    "benchmark_flag = 0\n",
    "for i in random_list: \n",
    "    if benchmark_flag == 1:\n",
    "        df = load_and_normalize_json(f\"{raw_Data_path}/{map_type}/LaCAM.json\")\n",
    "    else:\n",
    "        df = load_and_normalize_json(f\"/mnt/c/Users/Oguzhan Karaarslan/Desktop/repos/pogema-lacam-dcc/algorithms/experiments/{map_type}/LaCAM.json\")\n",
    "    \n",
    "    data_dict = {'LaCAM':{'Congestion':None}}\n",
    "    df = add_algorithm_prefix(df)\n",
    "    \n",
    "    if benchmark_flag == 1:\n",
    "        comb_df = get_combined_df(f\"{map_type}\",f\"{raw_Data_path}\")\n",
    "    else:\n",
    "        comb_df = get_combined_df(f\"{map_type}\",\"/mnt/c/Users/Oguzhan Karaarslan/Desktop/repos/pogema-lacam-dcc/algorithms/experiments\")\n",
    "    df_8 = comb_df[comb_df['env_grid_search.num_agents'] == i]\n",
    "\n",
    "    if benchmark_flag == 1:\n",
    "        add_congestion(data_dict, ['LaCAM'], df_8, f\"{raw_Data_path}/{map_type}/\")\n",
    "    else:\n",
    "        add_congestion(data_dict, ['LaCAM'], df_8, f\"/mnt/c/Users/Oguzhan Karaarslan/Desktop/repos/pogema-lacam-dcc/algorithms/experiments/{map_type}/\")\n",
    "\n",
    "    print(data_dict['LaCAM']['Congestion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dee0499d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8446329766370461\n",
      "0.9480064696651915\n",
      "1.0191500881197106\n"
     ]
    }
   ],
   "source": [
    "random_list_2 = [84]\n",
    "random_list_1 = [8,16,24]\n",
    "#random_list_3 = [ 32, 64, 96, 128, 160, 192 ]\n",
    "[232,256 ]\n",
    "random_list_4 = [ 64, 128, 192,256]\n",
    "random_list = random_list_1  \n",
    "raw_Data_path = \"/mnt/c/Users/Oguzhan Karaarslan/Desktop/repos/pogema-lacam-dcc/raw_data_MAPF\"\n",
    "map_type = '01-random'\n",
    "benchmark_flag = 0\n",
    "for i in random_list: \n",
    "    if benchmark_flag == 1:\n",
    "        df = load_and_normalize_json(f\"{raw_Data_path}/{map_type}/LaCAM.json\")\n",
    "    else:\n",
    "        df = load_and_normalize_json(f\"/mnt/c/Users/Oguzhan Karaarslan/Desktop/repos/pogema-lacam-dcc/algorithms/experiments/{map_type}/LaCAM.json\")\n",
    "    \n",
    "    data_dict = {'LaCAM':{'Congestion':None}}\n",
    "    df = add_algorithm_prefix(df)\n",
    "    \n",
    "    if benchmark_flag == 1:\n",
    "        comb_df = get_combined_df(f\"{map_type}\",f\"{raw_Data_path}\")\n",
    "    else:\n",
    "        comb_df = get_combined_df(f\"{map_type}\",\"/mnt/c/Users/Oguzhan Karaarslan/Desktop/repos/pogema-lacam-dcc/algorithms/experiments\")\n",
    "    df_8 = comb_df[comb_df['env_grid_search.num_agents'] == i]\n",
    "\n",
    "    if benchmark_flag == 1:\n",
    "        add_congestion(data_dict, ['LaCAM'], df_8, f\"{raw_Data_path}/{map_type}/\")\n",
    "    else:\n",
    "        add_congestion(data_dict, ['LaCAM'], df_8, f\"/mnt/c/Users/Oguzhan Karaarslan/Desktop/repos/pogema-lacam-dcc/algorithms/experiments/{map_type}/\")\n",
    "    #\n",
    "    print(data_dict['LaCAM']['Congestion']) # benchmark : 0.8319653741907043\n",
    "    # benchmark one map: 0.8183799272303593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a0954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##{'LaCAM': {'Congestion': 0.8327010122025179}} - \n",
    "##{'LaCAM': {'Congestion': 0.9476686737645119}} - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae783ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########    BENCHMARK   ########### \n",
    "### 8 ----- {'LaCAM': {'Congestion': 0.8313665077986148}}\n",
    "### 16 ----- {'LaCAM': {'Congestion': 0.9470853146846667}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aacef9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c71f555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f25bbd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2463b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
